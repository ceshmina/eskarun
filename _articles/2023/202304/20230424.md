---
title: Transformerのコンテキスト制限
---

月曜日だが、今週を乗り切ればGWなので頑張りたい。

夜に6月のライブの打ち合わせ。久しぶりにSkypeを使ったらチャットでBing AIが使えるようになっていたので、すこし動かしてみる。

---

今日の話題として、Transformerはコンテキスト制限が厳しかった (GPT-4も最大3万トークン) のだが、[Recurrent Memory Transformer](https://arxiv.org/abs/2207.06881)という枠組みで[BERTを200万トークンまで拡張する論文](https://arxiv.org/abs/2304.11062)が出ていた。GPUメモリを増やさずできると読み取ったが、精度がやや落ちるのと、実行時間は心配。近い将来GPT系にも応用されれば、また用途が一気に広がりそうだ。

流れでarXivを眺めていたら、[Transformerへの入門](https://arxiv.org/abs/2304.10557)みたいなレビュー？ があった。信頼できそうな所属の人なので、また読んでみたい。
